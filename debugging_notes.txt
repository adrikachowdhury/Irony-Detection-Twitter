1. Wrong tokenizer name
   Issue: The code tried to use 'punkt_tab', which doesn't exist in NLTK, causing errors.
   Fix: Replaced 'punkt_tab' and avoided using word_tokenize that relies on it.

2. word_tokenize needs punkt
   Issue: NLTK's word_tokenize depends on the 'punkt' tokenizer, which isn't installed by default on Streamlit Cloud.
   Fix: Replaced word_tokenize with TweetTokenizer, which works without downloading punkt.

3. Stopwords not found
   Issue: Using stopwords.words('english') caused errors because the stopwords corpus wasn't available in the cloud environment.
   Fix: Added nltk.download('stopwords', quiet=True) at runtime to ensure stopwords are available.

4. Using find() at the top
   Issue: Checking for NLTK files with nltk.data.find() sometimes caused NameError in the cloud.
   Fix: Removed find() calls entirely and handled downloads safely with nltk.download().

5. Cloud environment resets
   Issue: Streamlit Cloud deletes all downloaded NLTK files every time the app restarts, causing repeated errors.
   Fix: Ensured all necessary NLTK data (stopwords) is downloaded at app startup.

6. Multiple word_tokenize calls
   Issue: Using word_tokenize in multiple places increased chances of errors since it relies on punkt.
   Fix: Standardized tokenization with TweetTokenizer everywhere in preprocessing.

7. Mixed tokenizers
   Issue: Using both TweetTokenizer and word_tokenize caused confusion and instability on the cloud.
   Fix: Unified all tokenization to TweetTokenizer for consistency and cloud compatibility.
